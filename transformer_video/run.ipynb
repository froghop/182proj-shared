{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymunk in /Users/mashabondarenko/opt/anaconda3/lib/python3.8/site-packages (6.6.0)\n",
      "Requirement already satisfied: cffi>=1.15.0 in /Users/mashabondarenko/opt/anaconda3/lib/python3.8/site-packages (from pymunk) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /Users/mashabondarenko/opt/anaconda3/lib/python3.8/site-packages (from cffi>=1.15.0->pymunk) (2.20)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%run synthetic_data_generation.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABG0AAAB4CAYAAABIOg/8AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAEf0lEQVR4nO3dQU7bQBiAUU+VI9B1c4jc/wRwB7oud5iuWLWFgEz9jfXe2rJG+T2j6JMhY865AQAAANDy7egFAAAAAPAn0QYAAAAgSLQBAAAACBJtAAAAAIJEGwAAAIAg0QYAAAAg6PKRix8eHub1ev2ipfBZz8/P28vLy3jvOvPrenp6eplzfn/vOjNsuncPbpsZVjlH1+ccXZs9uD57cG2+y6zPObq+f52jH4o21+t1e3x83G9V7OJ2u911nfl1jTF+3nOdGTbduwe3zQyrnKPrc46uzR5cnz24Nt9l1uccXd+/zlF/HgUAAAAQJNoAAAAABIk2AAAAAEGiDQAAAECQaAMAAAAQJNoAAAAABIk2AAAAAEGiDQAAAECQaAMAAAAQJNoAAAAABF2OXsDZjTF2vd+cc9f7AQAAAE3etAEAAAAIEm0AAAAAgkQbAAAAgCDRBgAAACBItAEAAAAIEm0AAAAAgkQbAAAAgCDRBgAAACBItAEAAAAIEm0AAAAAgkQbAAAAgCDRBgAAACBItAEAAAAIEm0AAAAAgkQbAAAAgCDRBgAAACBItAEAAAAIEm0AAAAAgi5HL+Ds5pxHLwEAAABYkDdtAAAAAIJEGwAAAIAg0QYAAAAgSLQBAAAACBJtAAAAAIJEGwAAAIAg0QYAAAAgSLQBAAAACBJtAAAAAIJEGwAAAIAg0QYAAAAgSLQBAAAACBJtAAAAAIJEGwAAAIAg0QYAAAAgSLQBAAAACBJtAAAAAIJEGwAAAICgy9ELgLeMMXa715xzt3udwZ6f7bb5fAEAAPbmTRsAAACAINEGAAAAIEi0AQAAAAgSbQAAAACCRBsAAACAINEGAAAAIEi0AQAAAAgSbQAAAACCRBsAAACAINEGAAAAIEi0AQAAAAgSbQAAAACCRBsAAACAINEGAAAAIEi0AQAAAAgSbQAAAACCRBsAAACAINEGAAAAIEi0AQAAAAi6HL0AeMuc8+glnJbP9tzGGLvdy7OyBjMHADgfb9oAAAAABIk2AAAAAEGiDQAAAECQaAMAAAAQJNoAAAAABPn1KACAk/FrYrza81nYNs9Dzd7z5WvYhx0rzsKbNgAAAABBog0AAABAkGgDAAAAECTaAAAAAASJNgAAAABBog0AAABAkGgDAAAAECTaAAAAAASJNgAAAABBog0AAABAkGgDAAAAECTaAAAAAARdjl4AAPubcx69BP4zMwcAOB9v2gAAAAAEiTYAAAAAQaINAAAAQJBoAwAAABAk2gAAAAAEiTYAAAAAQX7yGwDgZPwEPK88C+e213xvt9su9+Hv7MOOFWfhTRsAAACAINEGAAAAIEi0AQAAAAgSbQAAAACCRBsAAACAoPGR/548xvi1bdvPr1sOn/Rjzvn9vYvML80M13bX/LbNDMPswfWZ4drMb31muDbfZdZnD67vrzP8ULQBAAAA4P/w51EAAAAAQaINAAAAQJBoAwAAABAk2gAAAAAEiTYAAAAAQaINAAAAQJBoAwAAABAk2gAAAAAEiTYAAAAAQb8BMkHiWIXnhA4AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1440x144 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate the sequence with the specified parameters\n",
    "sequence = generate_random_sequence(\n",
    "    sequence_length = 10,\n",
    "    frame_rate = 60,\n",
    "    speed_mean = 10,\n",
    "    speed_sd = 5,\n",
    "    direction_min = 0,\n",
    "    direction_max = 2 * math.pi,\n",
    "    position_x_mean = 8,\n",
    "    position_x_sd= 2,\n",
    "    position_y_mean = 8,\n",
    "    position_y_sd = 2,\n",
    "    gravity_mean = 9.81,\n",
    "    gravity_sd = 2,\n",
    "    restitution_min = 0.2,\n",
    "    restitution_max = 0.8\n",
    ")\n",
    "\n",
    "#sequence.append(create_question_mark_image(IMAGE_WIDTH, IMAGE_HEIGHT, 40))\n",
    "\n",
    "# Display the sequence\n",
    "display_sequence(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_n_sequences(n):\n",
    "\tsequences = []\n",
    "\tfor _ in range(n):\n",
    "\t\tsequence = generate_random_sequence(\n",
    "\t\tsequence_length = 10,\n",
    "\t\tframe_rate = 60,\n",
    "\t\tspeed_mean = 10,\n",
    "\t\tspeed_sd = 5,\n",
    "\t\tdirection_min = 0,\n",
    "\t\tdirection_max = 2 * math.pi,\n",
    "\t\tposition_x_mean = 8,\n",
    "\t\tposition_x_sd= 2,\n",
    "\t\tposition_y_mean = 8,\n",
    "\t\tposition_y_sd = 2,\n",
    "\t\tgravity_mean = 9.81,\n",
    "\t\tgravity_sd = 2,\n",
    "\t\trestitution_min = 0.2,\n",
    "\t\trestitution_max = 0.8\n",
    "\t\t)\n",
    "\t\tsequences.append(sequence)\n",
    "\treturn sequences\n",
    "images_sequences = create_n_sequences(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(images_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10, 16, 16, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "\n",
    "# Assuming you have a list of B sequences, each containing images of length L\n",
    "# For example, images_sequences = [sequence1, sequence2, ..., sequenceB]\n",
    "# Each sequence is a list of PIL images\n",
    "\n",
    "# Define a custom dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, sequences, transform=None):\n",
    "        self.sequences = sequences\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.sequences[idx]\n",
    "        if self.transform:\n",
    "            sequence = [self.transform(img) for img in sequence]\n",
    "        return torch.stack(sequence, dim=0)  # Stack images along the new dimension (sequence length)\n",
    "\n",
    "# Define the transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(),  # Assuming your images are grayscale\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Create a custom dataset\n",
    "# For example, images_sequences = [sequence1, sequence2, ..., sequenceB]\n",
    "dataset = CustomDataset(images_sequences, transform=transform)\n",
    "\n",
    "# Check if the dataset has a positive length\n",
    "if len(dataset) == 0:\n",
    "    raise ValueError(\"Dataset length is 0. Please check your data.\")\n",
    "\n",
    "# Create a data loader\n",
    "batch_size = 32  # Set your desired batch size\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Iterate over the data loader\n",
    "for batch in data_loader:\n",
    "    # Your training/validation loop goes here\n",
    "    batch = batch.permute(0, 1, 3, 4, 2)\n",
    "    print(batch.shape)  # This will print the shape of each batch of tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Module [Encoder] is missing the required \"forward\" function",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-6990f3de1f09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mVP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/CS182/proj/182proj-shared/transformer_video/video_prediction.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, inp, tar_seq_len)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtar_seq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             prediction, _ = self.transformer(\n\u001b[0m\u001b[1;32m    119\u001b[0m                 \u001b[0mencoder_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlook_ahead_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/CS182/proj/182proj-shared/transformer_video/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inp, tar, look_ahead_mask)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlook_ahead_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# enc_output.shape = (batch_size, inp_seq_len, rows, cols, d_model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0menc_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# dec_output.shape = (batch_size, tar_seq_len, rows, cols, d_model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_forward_unimplemented\u001b[0;34m(self, *input)\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0mregistered\u001b[0m \u001b[0mhooks\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mlatter\u001b[0m \u001b[0msilently\u001b[0m \u001b[0mignores\u001b[0m \u001b[0mthem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \"\"\"\n\u001b[0;32m--> 244\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Module [{type(self).__name__}] is missing the required \\\"forward\\\" function\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Module [Encoder] is missing the required \"forward\" function"
     ]
    }
   ],
   "source": [
    "from video_prediction import VideoPrediction\n",
    "\n",
    "\n",
    "VP = VideoPrediction(\n",
    "    num_layers=5, \n",
    "    d_model=128, \n",
    "    num_heads=16, \n",
    "    dff=128, \n",
    "    filter_size=3,\n",
    "    image_shape=(16,16), \n",
    "    pe_input=10, \n",
    "    pe_target=10, \n",
    "    out_channel=1,\n",
    "    loss_function='mse', \n",
    "    optimizer='rmsprop'\n",
    ")\n",
    "\n",
    "VP.predict(data_loader.dataset[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
